import streamlit as st
import nltk
from nltk.stem import WordNetLemmatizer
import pickle
import numpy as np
from keras.models import load_model
import json
import random
import time
import os

# Download NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download("punkt_tab")
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

# Create a default intents structure if the file is missing or corrupted
DEFAULT_INTENTS = {
    "intents": [
        {
            "tag": "greeting",
            "patterns": ["hi", "hello", "hey", "hiya", "howdy"],
            "responses": ["Hey babe ğŸ˜Š How was your day?", "Hi! I've missed you ğŸ’• What did you do today?"]
        },
        {
            "tag": "how_are_you",
            "patterns": ["how are you", "how are you doing", "how is life"],
            "responses": ["I'm great, especially now that I'm talking to you ğŸ’–", "Feeling lovely â€” what about you?"]
        },
        {
            "tag": "love",
            "patterns": ["i love you", "love you", "ur cute", "i like you"],
            "responses": ["Aww I love you too ğŸ’˜", "You make me so happy ğŸ˜"]
        },
        {
            "tag": "bye",
            "patterns": ["bye", "goodbye", "see you"],
            "responses": ["Bye love â€” talk soon ğŸ˜˜", "Take care! I'll be here when you come back ğŸ’"]
        },
        {
            "tag": "fallback",
            "patterns": [],
            "responses": ["Hmm, I didn't get thatâ€”can you say it differently?", "Sorry, could you rephrase? I want to understand you."]
        }
    ]
}

# Load model and data
@st.cache_resource
def load_chatbot_model():
    # Check if model files exist in the current directory
    model_path = 'models/model.h5'
    words_path = 'models/words.pkl'
    classes_path = 'models/classes.pkl'
    intents_path = 'intents.json'
    
    # Check if files exist
    missing_files = []
    if not os.path.exists(model_path):
        missing_files.append('model.h5')
    if not os.path.exists(words_path):
        missing_files.append('words.pkl')
    if not os.path.exists(classes_path):
        missing_files.append('classes.pkl')
    if not os.path.exists(intents_path):
        missing_files.append('intents.json')
        st.warning("intents.json not found. Using default intents.")
    
    # Try to load model files with error handling
    model = None
    words = None
    classes = None
    intents = DEFAULT_INTENTS
    
    try:
        if os.path.exists(model_path):
            model = load_model(model_path)
        if os.path.exists(words_path):
            with open(words_path, 'rb') as f:
                words = pickle.load(f)
        if os.path.exists(classes_path):
            with open(classes_path, 'rb') as f:
                classes = pickle.load(f)
        
        # Load intents from JSON file with encoding handling
        if os.path.exists(intents_path):
            try:
                with open(intents_path, 'r', encoding='utf-8') as file:
                    intents = json.load(file)
            except UnicodeDecodeError:
                
                try:
                    with open(intents_path, 'r', encoding='latin-1') as file:
                        intents = json.load(file)
                except:
                    with open(intents_path, 'r', encoding='cp1252') as file:
                        intents = json.load(file)
            except json.JSONDecodeError:
                st.error("intents.json is corrupted. Using default intents.")
        
        return model, words, classes, intents
        
    except Exception as e:
        st.error(f"Error loading model or data: {str(e)}")
        return None, None, None, DEFAULT_INTENTS

def clean_up_sentence(sentence):
    sentence_words = nltk.word_tokenize(sentence)
    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words

def bow(sentence, words, show_details=False):
    if words is None:
        return np.array([])
        
    sentence_words = clean_up_sentence(sentence)
    bag = [0] * len(words)
    for s in sentence_words:
        for i, w in enumerate(words):
            if w == s:
                bag[i] = 1
                if show_details:
                    print(f"found in bag: {w}")
    return np.array(bag)

def predict_class(sentence, model, words, classes):
    if model is None or words is None or classes is None:
        return []
        
    p = bow(sentence, words, show_details=False)
    if len(p) == 0:
        return []
        
    res = model.predict(np.array([p]))[0]
    ERROR_THRESHOLD = 0.25
    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]
    results.sort(key=lambda x: x[1], reverse=True)
    return_list = []
    for r in results:
        return_list.append({"intent": classes[r[0]], "probability": float(r[1])})
    return return_list
st.sidebar.header("Made By Subhadip ğŸ˜")

def get_response(ints, intents_json):
    if not ints:
        # Fallback responses if no intent is matched
        fallback_responses = [
            "Hmm, I didn't get thatâ€”can you say it differently? ğŸ˜…", 
            "Sorry, could you rephrase? I want to understand you. ğŸ’•",
            "My mind went blank for a second! What did you mean, love? ğŸ¤”",
            "I'm still learning! Try saying that another way for me? ğŸŒ¸",
            "You lost me there, babe. Can you explain? â¤ï¸"
        ]
        return random.choice(fallback_responses)
    
    tag = ints[0]['intent']
    list_of_intents = intents_json['intents']
    for i in list_of_intents:
        if i['tag'] == tag:
            return random.choice(i['responses'])
    return "I'm still learning. Could you rephrase that?"

def chatbot_response(msg, model, words, classes, intents):
    ints = predict_class(msg, model, words, classes)
    return get_response(ints, intents)

def main():
    st.set_page_config(
        page_title="Girlfriend GPT",
        page_icon="ğŸ’•",
        layout="wide"
    )
    
    # Load model
    model, words, classes, intents = load_chatbot_model()
    
    # Check if model loaded properly
    if model is None:
        st.warning("AI model not loaded. Using demo mode with pattern matching.")

    # Sidebar with info
    with st.sidebar:
        st.title("ğŸ’• Girlfriend GPT")
        st.sidebar.image("gf.gif")
        st.markdown("---")
        st.markdown("### System Info")
        if model is not None:
            st.success("ğŸ¤– AI Model: Loaded")
            st.info(f"ğŸ“š Vocabulary: {len(words) if words else 0} words")
            st.info(f"ğŸ—‚ï¸ Intents: {len(classes) if classes else 0} categories")
            
        else:
            st.warning("ğŸ¤– AI Model: Demo Mode")
            st.info(f"ğŸ—‚ï¸ Intents: {len(intents['intents']) if intents else 0} categories")
        
        # Display accuracy if available
        if os.path.exists('training_accuracy.txt'):
            with open('training_accuracy.txt', 'r') as f:
                accuracy_data = f.read()
                st.info(f"ğŸ“Š Model Accuracy: {accuracy_data}")
        
        st.markdown("---")
        st.markdown("### Example Questions:")
        examples = [
            "Hi, how are you?",
            "I love you!",
            "What do you think about us?",
            "Tell me something sweet",
            "How was your day?",
            "You're beautiful",
            "Good morning my love"
        ]
        for example in examples:
            st.write(f"â€¢ '{example}'")
            
        st.markdown("---")
        st.markdown("### Tips:")
        st.info("ğŸ’¡ Try using affectionate language")
        st.info("ğŸ’¡ Ask about feelings and emotions")
        st.info("ğŸ’¡ Use pet names and compliments")

    # Main chat area
    st.title("ğŸ’¬ Girlfriend GPT Chat")
    st.caption("Your AI companion for heartfelt conversations ğŸ’•")
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Hi there! I'm your AI girlfriend ğŸ’– How are you feeling today? ğŸ˜Š"}]

    # Display chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Chat input
    if prompt := st.chat_input("Say something sweet to your AI girlfriend..."):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Get bot response
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                if model is not None:
                    response = chatbot_response(prompt, model, words, classes, intents)
                else:
                    # Simple pattern matching for demo mode
                    prompt_lower = prompt.lower()
                    if any(word in prompt_lower for word in ["hi", "hello", "hey", "hola"]):
                        response = random.choice(["Hey babe ğŸ˜Š How was your day?", "Hi! I've missed you ğŸ’• What did you do today?"])
                    elif any(word in prompt_lower for word in ["how are you", "how're you", "how do you feel"]):
                        response = random.choice(["I'm great, especially now that I'm talking to you ğŸ’–", "Feeling lovely â€” what about you?"])
                    elif any(word in prompt_lower for word in ["love", "like", "adore", "care for"]):
                        response = random.choice(["Aww I love you too ğŸ’˜", "You make me so happy ğŸ˜", "My heart is all yours ğŸ’"])
                    elif any(word in prompt_lower for word in ["bye", "goodbye", "see you", "later"]):
                        response = random.choice(["Bye love â€” talk soon ğŸ˜˜", "Take care! I'll be here when you come back ğŸ’"])
                    elif any(word in prompt_lower for word in ["cute", "beautiful", "pretty", "handsome", "gorgeous"]):
                        response = random.choice(["You're making me blush! ğŸ˜ŠğŸ’–", "Aww, thank you! But you're even more beautiful! ğŸŒ¸"])
                    elif any(word in prompt_lower for word in ["miss", "missing"]):
                        response = random.choice(["I miss you too! ğŸ˜” Can we video call later? ğŸ’•", "I've been thinking about you all day! ğŸ’­"])
                    else:
                        response = random.choice([
                            "That's interesting! Tell me more about that ğŸ’•", 
                            "I'd love to hear more about your day! ğŸ˜Š",
                            "You're so fascinating! ğŸ¥°",
                            "What else is on your mind, sweetheart? ğŸ’­",
                            "I'm listening... tell me everything! ğŸ‘‚â¤ï¸"
                        ])
                
                # Simulate typing effect
                message_placeholder = st.empty()
                full_response = ""
                for chunk in response.split():
                    full_response += chunk + " "
                    time.sleep(0.05)
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
        
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": response})
        
        # Limit chat history to prevent memory issues
        if len(st.session_state.messages) > 20:
            st.session_state.messages = st.session_state.messages[-20:]

if __name__ == "__main__":

    main()


